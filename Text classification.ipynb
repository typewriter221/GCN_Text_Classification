{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Remove_words.py","execution_count":null},{"metadata":{"_uuid":"2e435f6e-ae41-43bc-8a45-44f848fcc3bd","_cell_guid":"740bc08c-2dbc-44ed-ba42-9219ed6c34c0","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\nfrom nltk.wsd import lesk\nfrom nltk.corpus import wordnet as wn\nfrom utils import clean_str, loadWord2Vec\nimport sys\n\n# if len(sys.argv) != 2:\n# \tsys.exit(\"Use: python remove_words.py <dataset>\")\n\ndatasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n# dataset = sys.argv[1]\n\n# if dataset not in datasets:\n# \tsys.exit(\"wrong dataset name\")\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n# Read Word Vectors\n# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n# word_embeddings_dim = len(embd[0])\ndataset = '20ng'\n\ndoc_content_list = []\nf = open('/kaggle/input/remove-words-20ng-txt/' + dataset + '.txt', 'rb')\n# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\nfor line in f.readlines():\n    doc_content_list.append(line.strip().decode('latin1'))\nf.close()\n\n\nword_freq = {}  # to remove rare words\n\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nclean_docs = []\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    doc_words = []\n    for word in words:\n        # word not in stop_words and word_freq[word] >= 5\n        if dataset == 'mr':\n            doc_words.append(word)\n        elif word not in stop_words and word_freq[word] >= 5:\n            doc_words.append(word)\n\n    doc_str = ' '.join(doc_words).strip()\n    #if doc_str == '':\n        #doc_str = temp\n    clean_docs.append(doc_str)\n\nclean_corpus_str = '\\n'.join(clean_docs)\n\nf = open('/kaggle/working/' + dataset + '.clean.txt', 'w')\n#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\n#print(\"Printing Clean_corpus_str :\" +  clean_corpus_str)\nf.write(clean_corpus_str)\nf.close()\n\n#dataset = '20ng'\nmin_len = 10000\naver_len = 0\nmax_len = 0 \n\nf = open('/kaggle/working/' + dataset + '.clean.txt', 'r')\n#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\nlines = f.readlines()\nfor line in lines:\n    line = line.strip()\n    temp = line.split()\n    aver_len = aver_len + len(temp)\n    if len(temp) < min_len:\n        min_len = len(temp)\n    if len(temp) > max_len:\n        max_len = len(temp)\nf.close()\naver_len = 1.0 * aver_len / len(lines)\nprint('min_len : ' + str(min_len))\nprint('max_len : ' + str(max_len))\nprint('average_len : ' + str(aver_len))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build_graph.py","execution_count":null},{"metadata":{"_uuid":"b4779e0e-947a-4499-9b2e-dfa81f43c4f4","_cell_guid":"a9b14abd-b960-41cc-a4ba-86f55bcde75e","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pickle as pkl\nimport networkx as nx\nimport scipy.sparse as sp\nfrom utils import loadWord2Vec, clean_str\nfrom math import log\nfrom sklearn import svm\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sys\nfrom scipy.spatial.distance import cosine\n\n# if len(sys.argv) != 2:\n# \tsys.exit(\"Use: python build_graph.py <dataset>\")\n\ndatasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n# build corpus\ndataset = '20ng'\n\nif dataset not in datasets:\n\tsys.exit(\"wrong dataset name\")\n\n# Read Word Vectors\n# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n# word_embeddings_dim = len(embd[0])\n\nword_embeddings_dim = 300\nword_vector_map = {}\n\n# shulffing\ndoc_name_list = []\ndoc_train_list = []\ndoc_test_list = []\n\nf = open('/kaggle/input/data-file/' + dataset + '.txt', 'r')  #change directory here\nlines = f.readlines()\nfor line in lines:\n    doc_name_list.append(line.strip())\n    temp = line.split(\"\\t\")\n    if temp[1].find('test') != -1:\n        doc_test_list.append(line.strip())\n    elif temp[1].find('train') != -1:\n        doc_train_list.append(line.strip())\nf.close()\n# print(doc_train_list)\n# print(doc_test_list)\n\ndoc_content_list = []\nf = open('/kaggle/input/data-file/' + dataset + '.clean.txt', 'r') #change directory here\nlines = f.readlines()\nfor line in lines:\n    doc_content_list.append(line.strip())\nf.close()\n# print(doc_content_list)\n\ntrain_ids = []\nfor train_name in doc_train_list:\n    train_id = doc_name_list.index(train_name)\n    train_ids.append(train_id)\nprint(train_ids)\nrandom.shuffle(train_ids)\n\n# partial labeled data\n#train_ids = train_ids[:int(0.2 * len(train_ids))]\n\ntrain_ids_str = '\\n'.join(str(index) for index in train_ids)\nf = open('/kaggle/working/' + dataset + '.train.index', 'w')   # No problem switch to Working Dir\nf.write(train_ids_str)\nf.close()\n\ntest_ids = []\nfor test_name in doc_test_list:\n    test_id = doc_name_list.index(test_name)\n    test_ids.append(test_id)\nprint(test_ids)\nrandom.shuffle(test_ids)\n\ntest_ids_str = '\\n'.join(str(index) for index in test_ids)\nf = open('/kaggle/working/' + dataset + '.test.index', 'w') #No problem switch to Working Dir\nf.write(test_ids_str)\nf.close()\n\nids = train_ids + test_ids\nprint(ids)\nprint(len(ids))\n\nshuffle_doc_name_list = []\nshuffle_doc_words_list = []\nfor id in ids:\n    shuffle_doc_name_list.append(doc_name_list[int(id)])\n    shuffle_doc_words_list.append(doc_content_list[int(id)])\nshuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\nshuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n\nf = open('/kaggle/working/' + dataset + '_shuffle.txt', 'w')\nf.write(shuffle_doc_name_str)\nf.close()\n\nf = open('/kaggle/working/' + dataset + '_shuffle.txt', 'w')\nf.write(shuffle_doc_words_str)\nf.close()\n\n# build vocab\nword_freq = {}\nword_set = set()\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    for word in words:\n        word_set.add(word)\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nvocab = list(word_set)\nvocab_size = len(vocab)\n\nword_doc_list = {}\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    appeared = set()\n    for word in words:\n        if word in appeared:\n            continue\n        if word in word_doc_list:\n            doc_list = word_doc_list[word]\n            doc_list.append(i)\n            word_doc_list[word] = doc_list\n        else:\n            word_doc_list[word] = [i]\n        appeared.add(word)\n\nword_doc_freq = {}\nfor word, doc_list in word_doc_list.items():\n    word_doc_freq[word] = len(doc_list)\n\nword_id_map = {}\nfor i in range(vocab_size):\n    word_id_map[vocab[i]] = i\n\nvocab_str = '\\n'.join(vocab)\n\nf = open('/kaggle/working/' + dataset + '_vocab.txt', 'w')\nf.write(vocab_str)\nf.close()\n\n\nWord definitions begin\n\n\ndefinitions = []\n\nfor word in vocab:\n    word = word.strip()\n    synsets = wn.synsets(clean_str(word))\n    word_defs = []\n    for synset in synsets:\n        syn_def = synset.definition()\n        word_defs.append(syn_def)\n    word_des = ' '.join(word_defs)\n    if word_des == '':\n        word_des = '<PAD>'\n    definitions.append(word_des)\n\nstring = '\\n'.join(definitions)\n\n\nf = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\nf.write(string)\nf.close()\n\ntfidf_vec = TfidfVectorizer(max_features=1000)\ntfidf_matrix = tfidf_vec.fit_transform(definitions)\ntfidf_matrix_array = tfidf_matrix.toarray()\nprint(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n\nword_vectors = []\n\nfor i in range(len(vocab)):\n    word = vocab[i]\n    vector = tfidf_matrix_array[i]\n    str_vector = []\n    for j in range(len(vector)):\n        str_vector.append(str(vector[j]))\n    temp = ' '.join(str_vector)\n    word_vector = word + ' ' + temp\n    word_vectors.append(word_vector)\n\nstring = '\\n'.join(word_vectors)\n\nf = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\nf.write(string)\nf.close()\n\nword_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n_, embd, word_vector_map = loadWord2Vec(word_vector_file)\nword_embeddings_dim = len(embd[0])\n\n\n'''\nWord definitions end\n'''\n\n# label list\nlabel_set = set()\nfor doc_meta in shuffle_doc_name_list:\n    temp = doc_meta.split('\\t')\n    label_set.add(temp[2])\nlabel_list = list(label_set)\n\nlabel_list_str = '\\n'.join(label_list)\nf = open('/kaggle/working/' + dataset + '_labels.txt', 'w')\nf.write(label_list_str)\nf.close()\n\n# x: feature vectors of training docs, no initial features\n# slect 90% training set\ntrain_size = len(train_ids)\nval_size = int(0.1 * train_size)\nreal_train_size = train_size - val_size  # - int(0.5 * train_size)\n# different training rates\n\nreal_train_doc_names = shuffle_doc_name_list[:real_train_size]\nreal_train_doc_names_str = '\\n'.join(real_train_doc_names)\n\nf = open('/kaggle/working/' + dataset + '.real_train.name', 'w')\nf.write(real_train_doc_names_str)\nf.close()\n\nrow_x = []\ncol_x = []\ndata_x = []\nfor i in range(real_train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            # print(doc_vec)\n            # print(np.array(word_vector))\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_x.append(i)\n        col_x.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n\n# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\nx = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n    real_train_size, word_embeddings_dim))\n\ny = []\nfor i in range(real_train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    y.append(one_hot)\ny = np.array(y)\nprint(y)\n\n# tx: feature vectors of test docs, no initial features\ntest_size = len(test_ids)\n\nrow_tx = []\ncol_tx = []\ndata_tx = []\nfor i in range(test_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i + train_size]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_tx.append(i)\n        col_tx.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n\n# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\ntx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n                   shape=(test_size, word_embeddings_dim))\n\nty = []\nfor i in range(test_size):\n    doc_meta = shuffle_doc_name_list[i + train_size]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ty.append(one_hot)\nty = np.array(ty)\nprint(ty)\n\n# allx: the the feature vectors of both labeled and unlabeled training instances\n# (a superset of x)\n# unlabeled training instances -> words\n\nword_vectors = np.random.uniform(-0.01, 0.01,\n                                 (vocab_size, word_embeddings_dim))\n\nfor i in range(len(vocab)):\n    word = vocab[i]\n    if word in word_vector_map:\n        vector = word_vector_map[word]\n        word_vectors[i] = vector\n\nrow_allx = []\ncol_allx = []\ndata_allx = []\n\nfor i in range(train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i))\n        col_allx.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\nfor i in range(vocab_size):\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i + train_size))\n        col_allx.append(j)\n        data_allx.append(word_vectors.item((i, j)))\n\n\nrow_allx = np.array(row_allx)\ncol_allx = np.array(col_allx)\ndata_allx = np.array(data_allx)\n\nallx = sp.csr_matrix(\n    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n\nally = []\nfor i in range(train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ally.append(one_hot)\n\nfor i in range(vocab_size):\n    one_hot = [0 for l in range(len(label_list))]\n    ally.append(one_hot)\n\nally = np.array(ally)\n\nprint(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n''''''\nDoc word heterogeneous graph\n''''''\n\n# word co-occurence with context windows\nwindow_size = 20\nwindows = []\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    length = len(words)\n    if length <= window_size:\n        windows.append(words)\n    else:\n        # print(length, length - window_size + 1)\n        for j in range(length - window_size + 1):\n            window = words[j: j + window_size]\n            windows.append(window)\n            # print(window)\n\n\nword_window_freq = {}\nfor window in windows:\n    appeared = set()\n    for i in range(len(window)):\n        if window[i] in appeared:\n            continue\n        if window[i] in word_window_freq:\n            word_window_freq[window[i]] += 1\n        else:\n            word_window_freq[window[i]] = 1\n        appeared.add(window[i])\n\nword_pair_count = {}\nfor window in windows:\n    for i in range(1, len(window)):\n        for j in range(0, i):\n            word_i = window[i]\n            word_i_id = word_id_map[word_i]\n            word_j = window[j]\n            word_j_id = word_id_map[word_j]\n            if word_i_id == word_j_id:\n                continue\n            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n            # two orders\n            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n\nrow = []\ncol = []\nweight = []\n\n# pmi as weights\n\nnum_window = len(windows)\n\nfor key in word_pair_count:\n    temp = key.split(',')\n    i = int(temp[0])\n    j = int(temp[1])\n    count = word_pair_count[key]\n    word_freq_i = word_window_freq[vocab[i]]\n    word_freq_j = word_window_freq[vocab[j]]\n    pmi = log((1.0 * count / num_window) /\n              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n    if pmi <= 0:\n        continue\n    row.append(train_size + i)\n    col.append(train_size + j)\n    weight.append(pmi)\n\n# word vector cosine similarity as weights\n\n\nfor i in range(vocab_size):\n    for j in range(vocab_size):\n        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n            vector_i = np.array(word_vector_map[vocab[i]])\n            vector_j = np.array(word_vector_map[vocab[j]])\n            similarity = 1.0 - cosine(vector_i, vector_j)\n            if similarity > 0.9:\n                print(vocab[i], vocab[j], similarity)\n                row.append(train_size + i)\n                col.append(train_size + j)\n                weight.append(similarity)\n\n# doc word frequency\ndoc_word_freq = {}\n\nfor doc_id in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[doc_id]\n    words = doc_words.split()\n    for word in words:\n        word_id = word_id_map[word]\n        doc_word_str = str(doc_id) + ',' + str(word_id)\n        if doc_word_str in doc_word_freq:\n            doc_word_freq[doc_word_str] += 1\n        else:\n            doc_word_freq[doc_word_str] = 1\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_word_set = set()\n    for word in words:\n        if word in doc_word_set:\n            continue\n        j = word_id_map[word]\n        key = str(i) + ',' + str(j)\n        freq = doc_word_freq[key]\n        if i < train_size:\n            row.append(i)\n        else:\n            row.append(i + vocab_size)\n        col.append(train_size + j)\n        idf = log(1.0 * len(shuffle_doc_words_list) /\n                  word_doc_freq[vocab[j]])\n        weight.append(freq * idf)\n        doc_word_set.add(word)\n\nnode_size = train_size + vocab_size + test_size\nadj = sp.csr_matrix(\n    (weight, (row, col)), shape=(node_size, node_size))\n\n# dump objects\nf = open(\"/kaggle/working/ind.{}.x\".format(dataset), 'wb')\npkl.dump(x, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.y\".format(dataset), 'wb')\npkl.dump(y, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.tx\".format(dataset), 'wb')\npkl.dump(tx, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.ty\".format(dataset), 'wb')\npkl.dump(ty, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.allx\".format(dataset), 'wb')\npkl.dump(allx, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.ally\".format(dataset), 'wb')\npkl.dump(ally, f)\nf.close()\n\nf = open(\"/kaggle/working/ind.{}.adj\".format(dataset), 'wb')\npkl.dump(adj, f)\nf.close()'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2416d9a9-2861-40dc-b8da-0d2f28d32238","_cell_guid":"411e0007-153a-4e4d-9ca5-a256faf38a20","trusted":true,"collapsed":true},"cell_type":"code","source":"# Depreciating tensorflow version to 1.15\n!pip install tensorflow==1.15.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# init.py code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load ../input/utils-py/inits.py\nimport tensorflow as tf\nimport numpy as np\n\n\ndef uniform(shape, scale=0.05, name=None):\n    \"\"\"Uniform init.\"\"\"\n    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef glorot(shape, name=None):\n    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef zeros(shape, name=None):\n    \"\"\"All zeros.\"\"\"\n    initial = tf.zeros(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef ones(shape, name=None):\n    \"\"\"All ones.\"\"\"\n    initial = tf.ones(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Layers.py code****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load ../input/utils-py/layers.py\n#from inits import *\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# global unique layer ID dictionary for layer name assignment\n_LAYER_UIDS = {}\n\n\ndef get_layer_uid(layer_name=''):\n    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n    if layer_name not in _LAYER_UIDS:\n        _LAYER_UIDS[layer_name] = 1\n        return 1\n    else:\n        _LAYER_UIDS[layer_name] += 1\n        return _LAYER_UIDS[layer_name]\n\n\ndef sparse_dropout(x, keep_prob, noise_shape):\n    \"\"\"Dropout for sparse tensors.\"\"\"\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse_retain(x, dropout_mask)\n    return pre_out * (1./keep_prob)\n\n\ndef dot(x, y, sparse=False):\n    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n    if sparse:\n        res = tf.sparse_tensor_dense_matmul(x, y)\n    else:\n        res = tf.matmul(x, y)\n    return res\n\n\nclass Layer(object):\n    \"\"\"Base layer class. Defines basic API for all layer objects.\n    Implementation inspired by keras (http://keras.io).\n\n    # Properties\n        name: String, defines the variable scope of the layer.\n        logging: Boolean, switches Tensorflow histogram logging on/off\n\n    # Methods\n        _call(inputs): Defines computation graph of layer\n            (i.e. takes input, returns output)\n        __call__(inputs): Wrapper for _call()\n        _log_vars(): Log all variables\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            layer = self.__class__.__name__.lower()\n            name = layer + '_' + str(get_layer_uid(layer))\n        self.name = name\n        self.vars = {}\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n        self.sparse_inputs = False\n\n    def _call(self, inputs):\n        return inputs\n\n    def __call__(self, inputs):\n        with tf.name_scope(self.name):\n            if self.logging and not self.sparse_inputs:\n                tf.summary.histogram(self.name + '/inputs', inputs)\n            outputs = self._call(inputs)\n            if self.logging:\n                tf.summary.histogram(self.name + '/outputs', outputs)\n            return outputs\n\n    def _log_vars(self):\n        for var in self.vars:\n            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n\n\nclass Dense(Layer):\n    \"\"\"Dense layer.\"\"\"\n    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n        super(Dense, self).__init__(**kwargs)\n\n        if dropout:\n            self.dropout = placeholders['dropout']\n        else:\n            self.dropout = 0.\n\n        self.act = act\n        self.sparse_inputs = sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n\n        # helper variable for sparse dropout\n        self.num_features_nonzero = placeholders['num_features_nonzero']\n\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['weights'] = glorot([input_dim, output_dim],\n                                          name='weights')\n            if self.bias:\n                self.vars['bias'] = zeros([output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n    def _call(self, inputs):\n        x = inputs\n\n        # dropout\n        if self.sparse_inputs:\n            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n        else:\n            x = tf.nn.dropout(x, 1-self.dropout)\n\n        # transform\n        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n\n        return self.act(output)\n\n\nclass GraphConvolution(Layer):\n    \"\"\"Graph convolution layer.\"\"\"\n    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n                 featureless=False, **kwargs):\n        super(GraphConvolution, self).__init__(**kwargs)\n\n        if dropout:\n            self.dropout = placeholders['dropout']\n        else:\n            self.dropout = 0.\n\n        self.act = act\n        self.support = placeholders['support']\n        self.sparse_inputs = sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n\n        # helper variable for sparse dropout\n        self.num_features_nonzero = placeholders['num_features_nonzero']\n\n        with tf.variable_scope(self.name + '_vars'):\n            for i in range(len(self.support)):\n                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n                                                        name='weights_' + str(i))\n            if self.bias:\n                self.vars['bias'] = zeros([output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n    def _call(self, inputs):\n        x = inputs\n\n        # dropout\n        if self.sparse_inputs:\n            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n        else:\n            x = tf.nn.dropout(x, 1-self.dropout)\n\n        # convolve\n        supports = list()\n        for i in range(len(self.support)):\n            if not self.featureless:\n                pre_sup = dot(x, self.vars['weights_' + str(i)],\n                              sparse=self.sparse_inputs)\n            else:\n                pre_sup = self.vars['weights_' + str(i)]\n            support = dot(self.support[i], pre_sup, sparse=True)\n            supports.append(support)\n        output = tf.add_n(supports)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n        self.embedding = output #output\n        return self.act(output)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics.py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load ../input/utils-py/metrics.py\nimport tensorflow as tf\n\ndef masked_softmax_cross_entropy(preds, labels, mask):\n    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n    print(preds)\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    loss *= mask\n    return tf.reduce_mean(loss)\n\n\ndef masked_accuracy(preds, labels, mask):\n    \"\"\"Accuracy with masking.\"\"\"\n    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n\n    accuracy_all = tf.cast(correct_prediction, tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    accuracy_all *= mask\n    return tf.reduce_mean(accuracy_all)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models.py code-","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load models.py\n#from input.utils-py.layers import *\n#from metrics import *\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n\nclass Model(object):\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            name = self.__class__.__name__.lower()\n        self.name = name\n\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n\n        self.vars = {}\n        self.placeholders = {}\n\n        self.layers = []\n        self.activations = []\n\n        self.inputs = None\n        self.outputs = None\n\n        self.loss = 0\n        self.accuracy = 0\n        self.optimizer = None\n        self.opt_op = None\n\n    def _build(self):\n        raise NotImplementedError\n\n    def build(self):\n        \"\"\" Wrapper for _build() \"\"\"\n        with tf.variable_scope(self.name):\n            self._build()\n\n        # Build sequential layer model\n        self.activations.append(self.inputs)\n        for layer in self.layers:\n            hidden = layer(self.activations[-1])\n            self.activations.append(hidden)\n        self.outputs = self.activations[-1]\n\n        # Store model variables for easy access\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n        self.vars = {var.name: var for var in variables}\n\n        # Build metrics\n        self._loss()\n        self._accuracy()\n\n        self.opt_op = self.optimizer.minimize(self.loss)\n\n    def predict(self):\n        pass\n\n    def _loss(self):\n        raise NotImplementedError\n\n    def _accuracy(self):\n        raise NotImplementedError\n\n    def save(self, sess=None):\n        if not sess:\n            raise AttributeError(\"TensorFlow session not provided.\")\n        saver = tf.train.Saver(self.vars)\n        save_path = saver.save(sess, \"../working/tmp/%s.ckpt\" % self.name)\n        print(\"Model saved in file: %s\" % save_path)\n\n    def load(self, sess=None):\n        if not sess:\n            raise AttributeError(\"TensorFlow session not provided.\")\n        saver = tf.train.Saver(self.vars)\n        save_path = \"../working/tmp/%s.ckpt\" % self.name\n        saver.restore(sess, save_path)\n        print(\"Model restored from file: %s\" % save_path)\n\n\nclass MLP(Model):\n    def __init__(self, placeholders, input_dim, **kwargs):\n        super(MLP, self).__init__(**kwargs)\n\n        self.inputs = placeholders['features']\n        self.input_dim = input_dim\n        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n        self.placeholders = placeholders\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n\n        self.build()\n\n    def _loss(self):\n        # Weight decay loss\n        for var in self.layers[0].vars.values():\n            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n                                                  self.placeholders['labels_mask'])\n\n    def _accuracy(self):\n        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n                                        self.placeholders['labels_mask'])\n\n    def _build(self):\n        self.layers.append(Dense(input_dim=self.input_dim,\n                                 output_dim=FLAGS.hidden1,\n                                 placeholders=self.placeholders,\n                                 act=tf.nn.relu,\n                                 dropout=True,\n                                 sparse_inputs=True,\n                                 logging=self.logging))\n\n        self.layers.append(Dense(input_dim=FLAGS.hidden1,\n                                 output_dim=self.output_dim,\n                                 placeholders=self.placeholders,\n                                 act=lambda x: x,\n                                 dropout=True,\n                                 logging=self.logging))\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)\n\n\nclass GCN(Model):\n    def __init__(self, placeholders, input_dim, **kwargs):\n        super(GCN, self).__init__(**kwargs)\n\n        self.inputs = placeholders['features']\n        self.input_dim = input_dim\n        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n        self.placeholders = placeholders\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n\n        self.build()\n\n    def _loss(self):\n        # Weight decay loss\n        for var in self.layers[0].vars.values():\n            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n                                                  self.placeholders['labels_mask'])\n\n    def _accuracy(self):\n        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n                                        self.placeholders['labels_mask'])\n        self.pred = tf.argmax(self.outputs, 1)\n        self.labels = tf.argmax(self.placeholders['labels'], 1)\n\n    def _build(self):\n\n        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n                                            output_dim=FLAGS.hidden1,\n                                            placeholders=self.placeholders,\n                                            act=tf.nn.relu,\n                                            dropout=True,\n                                            featureless=True,\n                                            sparse_inputs=True,\n                                            logging=self.logging))\n\n        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n                                            output_dim=self.output_dim,\n                                            placeholders=self.placeholders,\n                                            act=lambda x: x, #\n                                            dropout=True,\n                                            logging=self.logging))\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils.py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %load utils.py\nimport numpy as np\nimport pickle as pkl\nimport networkx as nx\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg.eigen.arpack import eigsh\nimport sys\nimport re\n\n\ndef parse_index_file(filename):\n    \"\"\"Parse index file.\"\"\"\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\n\ndef sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n\ndef load_data(dataset_str):\n    \"\"\"\n    Loads input data from gcn/data directory\n\n    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n        object;\n    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n\n    All objects above must be saved using python pickle module.\n\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    \"\"\"\n    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n    objects = []\n    for i in range(len(names)):\n        with open(\"../input/build-data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding='latin1'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, graph = tuple(objects)\n    test_idx_reorder = parse_index_file(\n        \"../input/build-data/ind.{}.test.index\".format(dataset_str))\n    test_idx_range = np.sort(test_idx_reorder)\n    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n    # training nodes are training docs, no initial features\n    # print(\"x: \", x)\n    # test nodes are training docs, no initial features\n    # print(\"tx: \", tx)\n    # both labeled and unlabeled training instances are training docs and words\n    # print(\"allx: \", allx)\n    # training labels are training doc labels\n    # print(\"y: \", y)\n    # test labels are test doc labels\n    # print(\"ty: \", ty)\n    # ally are labels for labels for allx, some will not have labels, i.e., all 0\n    # print(\"ally: \\n\")\n    # for i in ally:\n    # if(sum(i) == 0):\n    # print(i)\n    # graph edge weight is the word co-occurence or doc word frequency\n    # no need to build map, directly build csr_matrix\n    # print('graph : ', graph)\n\n    if dataset_str == 'citeseer':\n        # Fix citeseer dataset (there are some isolated nodes in the graph)\n        # Find isolated nodes, add them as zero-vecs into the right position\n        test_idx_range_full = range(\n            min(test_idx_reorder), max(test_idx_reorder)+1)\n        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n        tx = tx_extended\n        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n        ty = ty_extended\n\n    features = sp.vstack((allx, tx)).tolil()\n    features[test_idx_reorder, :] = features[test_idx_range, :]\n    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n    labels = np.vstack((ally, ty))\n    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n    # print(len(labels))\n\n    idx_test = test_idx_range.tolist()\n    # print(idx_test)\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y)+500)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\n\ndef load_corpus(dataset_str):\n    \"\"\"\n    Loads input corpus from gcn/data directory\n\n    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.train.index => the indices of training docs in original doc list.\n\n    All objects above must be saved using python pickle module.\n\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    \"\"\"\n\n    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n    objects = []\n    for i in range(len(names)):\n        with open(\"../input/build-data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding='latin1'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, adj = tuple(objects)\n    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n    features = sp.vstack((allx, tx)).tolil()\n    labels = np.vstack((ally, ty))\n    print(len(labels))\n\n    train_idx_orig = parse_index_file(\n        \"../input/build-data/{}.train.index\".format(dataset_str))\n    train_size = len(train_idx_orig)\n\n    val_size = train_size - x.shape[0]\n    test_size = tx.shape[0]\n\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y) + val_size)\n    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n\n\ndef sparse_to_tuple(sparse_mx):\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\n\ndef preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return sparse_to_tuple(features)\n\n\ndef normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n\ndef preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    return sparse_to_tuple(adj_normalized)\n\n\ndef construct_feed_dict(features, support, labels, labels_mask, placeholders):\n    \"\"\"Construct feed dictionary.\"\"\"\n    feed_dict = dict()\n    feed_dict.update({placeholders['labels']: labels})\n    feed_dict.update({placeholders['labels_mask']: labels_mask})\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['support'][i]: support[i]\n                      for i in range(len(support))})\n    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n    return feed_dict\n\n\ndef chebyshev_polynomials(adj, k):\n    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n\n    adj_normalized = normalize_adj(adj)\n    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n    scaled_laplacian = (\n        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n\n    t_k = list()\n    t_k.append(sp.eye(adj.shape[0]))\n    t_k.append(scaled_laplacian)\n\n    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n\n    for i in range(2, k+1):\n        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n\n    return sparse_to_tuple(t_k)\n\n\ndef loadWord2Vec(filename):\n    \"\"\"Read Word Vectors\"\"\"\n    vocab = []\n    embd = []\n    word_vector_map = {}\n    file = open(filename, 'r')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        if(len(row) > 2):\n            vocab.append(row[0])\n            vector = row[1:]\n            length = len(vector)\n            for i in range(length):\n                vector[i] = float(vector[i])\n            embd.append(vector)\n            word_vector_map[row[0]] = vector\n    print('Loaded Word Vectors!')\n    file.close()\n    return vocab, embd, word_vector_map\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train.py","execution_count":null},{"metadata":{"_uuid":"89d2a72a-26ec-4650-951d-f41ab5cb4331","_cell_guid":"14d75251-15c4-40ce-8047-ebcde89e824a","trusted":true,"collapsed":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\n\nfrom sklearn import metrics\n#from utils import *\n#from models import GCN, MLP\nimport random\nimport os\nimport sys\n\n# if len(sys.argv) != 2:\n# \tsys.exit(\"Use: python train.py <dataset>\")\n\ndatasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\ndataset = '20ng'\n\nif dataset not in datasets:\n\tsys.exit(\"wrong dataset name\")\n\n\n# Set random seed\nseed = random.randint(1, 200)\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Settings\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n#to fix flag error because of tensorflow 1.15\nremaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\nassert(remaining_args == [sys.argv[0]])\n\ndef del_all_flags(FLAGS):\n    flags_dict = FLAGS._flags()    \n    keys_list = [keys for keys in flags_dict]    \n    for keys in keys_list:\n        FLAGS.__delattr__(keys)\n\ndel_all_flags(tf.flags.FLAGS)\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n# 'cora', 'citeseer', 'pubmed'\nflags.DEFINE_string('dataset', dataset, 'Dataset string.')\n# 'gcn', 'gcn_cheby', 'dense'\nflags.DEFINE_string('model', 'gcn', 'Model string.')\nflags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\nflags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\nflags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\nflags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\nflags.DEFINE_float('weight_decay', 0,\n                   'Weight for L2 loss on embedding matrix.')  # 5e-4\nflags.DEFINE_integer('early_stopping', 10,\n                     'Tolerance for early stopping (# of epochs).')\nflags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n\n# Load data\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n    FLAGS.dataset)\nprint(adj)\n# print(adj[0], adj[1])\nfeatures = sp.identity(features.shape[0])  # featureless\n\nprint(adj.shape)\nprint(features.shape)\n\n# Some preprocessing\nfeatures = preprocess_features(features)\nif FLAGS.model == 'gcn':\n    support = [preprocess_adj(adj)]\n    num_supports = 1\n    model_func = GCN\nelif FLAGS.model == 'gcn_cheby':\n    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n    num_supports = 1 + FLAGS.max_degree\n    model_func = GCN\nelif FLAGS.model == 'dense':\n    support = [preprocess_adj(adj)]  # Not used\n    num_supports = 1\n    model_func = MLP\nelse:\n    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n\n# Define placeholders\nplaceholders = {\n    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n    'labels_mask': tf.placeholder(tf.int32),\n    'dropout': tf.placeholder_with_default(0., shape=()),\n    # helper variable for sparse dropout\n    'num_features_nonzero': tf.placeholder(tf.int32)\n}\n\n# Create model\nprint(features[2][1])\nmodel = model_func(placeholders, input_dim=features[2][1], logging=True)\n\n# Initialize session\nsession_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\nsess = tf.Session(config=session_conf)\n\n\n# Define model evaluation function\ndef evaluate(features, support, labels, mask, placeholders):\n    t_test = time.time()\n    feed_dict_val = construct_feed_dict(\n        features, support, labels, mask, placeholders)\n    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n\n\n# Init variables\nsess.run(tf.global_variables_initializer())\n\ncost_val = []\n\n# Train model\nfor epoch in range(FLAGS.epochs):\n\n    t = time.time()\n    # Construct feed dictionary\n    feed_dict = construct_feed_dict(\n        features, support, y_train, train_mask, placeholders)\n    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n\n    # Training step\n    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n                     model.layers[0].embedding], feed_dict=feed_dict)\n\n    # Validation\n    cost, acc, pred, labels, duration = evaluate(\n        features, support, y_val, val_mask, placeholders)\n    cost_val.append(cost)\n\n    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n          \"train_acc=\", \"{:.5f}\".format(\n              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n\n    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n        print(\"Early stopping...\")\n        break\n\nprint(\"Optimization Finished!\")\n\n# Testing\ntest_cost, test_acc, pred, labels, test_duration = evaluate(\n    features, support, y_test, test_mask, placeholders)\nprint(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n\ntest_pred = []\ntest_labels = []\nprint(len(test_mask))\nfor i in range(len(test_mask)):\n    if test_mask[i]:\n        test_pred.append(pred[i])\n        test_labels.append(labels[i])\n\nprint(\"Test Precision, Recall and F1-Score...\")\nprint(metrics.classification_report(test_labels, test_pred, digits=4))\nprint(\"Macro average Test Precision, Recall and F1-Score...\")\nprint(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\nprint(\"Micro average Test Precision, Recall and F1-Score...\")\nprint(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n\n# doc and word embeddings\nprint('embeddings:')\nword_embeddings = outs[3][train_size: adj.shape[0] - test_size]\ntrain_doc_embeddings = outs[3][:train_size]  # include val docs\ntest_doc_embeddings = outs[3][adj.shape[0] - test_size:]\n\nprint(len(word_embeddings), len(train_doc_embeddings),\n      len(test_doc_embeddings))\nprint(word_embeddings)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('../input/vocabulary/' + dataset + '_vocab.txt', 'r')\nwords = f.readlines()\nf.close()\n\nvocab_size = len(words)\nword_vectors = []\nfor i in range(vocab_size):\n    word = words[i].strip()\n    word_vector = word_embeddings[i]\n    word_vector_str = ' '.join([str(x) for x in word_vector])\n    word_vectors.append(word + ' ' + word_vector_str)\n\nword_embeddings_str = '\\n'.join(word_vectors)\nf = open('/kaggle/working/' + dataset + '_word_vectors.txt', 'w')\nf.write(word_embeddings_str)\nf.close()\n\ndoc_vectors = []\ndoc_id = 0\nfor i in range(train_size):\n    doc_vector = train_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\nfor i in range(test_size):\n    doc_vector = test_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\ndoc_embeddings_str = '\\n'.join(doc_vectors)\nf = open('/kaggle/working/' + dataset + '_doc_vectors.txt', 'w')\nf.write(doc_embeddings_str)\nf.close()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}